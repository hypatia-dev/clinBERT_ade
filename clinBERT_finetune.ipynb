{"cells":[{"cell_type":"markdown","metadata":{"id":"_PRKTipexiUz"},"source":["# Fine-Tuning ClinicalBERT on ADE Text Classification\n","\n","This notebook applies a fine-tuned ClinicalBERT model for classifying adverse drug events (ADEs) in medical texts. It is trained on the ADE-Corpus-V2 Dataset: Adverse Drug Reaction Data. This is a dataset for Classification if a sentence is ADE-related (True=1) or not (False=0)."]},{"cell_type":"markdown","source":["#Install Prerequisites"],"metadata":{"id":"L6TjensaoPa7"}},{"cell_type":"code","source":["# Mount onto drive\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n","\n","%cd '/content/drive/MyDrive/GaTech/bert/'"],"metadata":{"id":"54du-zOvEQMU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXVE6-laxiU1","collapsed":true},"source":["# Install required packages if needed\n","!pip install -r requirements.txt\n","!pip install torch -U"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGaPz8MRxiU3"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    AutoModel,\n","    AdamW,\n","    get_linear_schedule_with_warmup\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# Clear cuda cache\n","torch.cuda.empty_cache()"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MiHfeLRxxiU7"},"source":["# Create Tokenizer Class\n","Tokenization prepares the strings from the dataset into units called \"tokens\", which streamlines the way in which the model learns from this data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82_SnGPmxiU7"},"source":["class TokenizeDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        # If tweaking of the tokenizer is required:\n","        # https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.padding\n","        encoding = self.tokenizer(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt')\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9xugKRvxiU8"},"source":["# Define Training and Evaluation Functions"]},{"cell_type":"code","source":["def plot_training_metrics(train_losses, val_losses, train_perplexities, val_perplexities, train_accuracies, val_accuracies, epochs):\n","    \"\"\"Plot training and validation metrics.\"\"\"\n","\n","    epochs_range = range(1, epochs + 1)\n","\n","    # Plot losses\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(epochs_range, train_losses, 'b-', label='Training Loss')\n","    plt.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\n","    plt.title('ClinicalBERT Training and Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.savefig('ClinicalBERTLoss.png')\n","    plt.show()\n","    plt.close()\n","\n","    # Plot perplexities\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(epochs_range, train_perplexities, 'b-', label='Training Perplexity')\n","    plt.plot(epochs_range, val_perplexities, 'r-', label='Validation Perplexity')\n","    plt.title('ClinicalBERT Training and Validation Perplexity')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Perplexity')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.savefig('ClinicalBERTPerplexity.png')\n","    plt.show()\n","    plt.close()\n","\n","    # Plot accuracies\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(epochs_range, train_accuracies, 'b-', label='Training Accuracy')\n","    plt.plot(epochs_range, val_accuracies, 'r-', label='Validation Accuracy')\n","    plt.title('ClinicalBERT Training and Validation Accuracies')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.savefig('ClinicalBERTAccuracy.png')\n","    plt.show()\n","    plt.close()\n","\n","    plt.show()"],"metadata":{"id":"S_9tcC2Hs4gz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhJxnIY7xiU9"},"source":["def train_epoch(model, data_loader, optimizer, scheduler, device):\n","    model.train()\n","    total_loss = 0\n","    predictions = []\n","    actual_labels = []\n","\n","    for batch in data_loader:\n","        optimizer.zero_grad()\n","\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        predictions.extend(preds.cpu().tolist())\n","        actual_labels.extend(labels.cpu().tolist())\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","\n","    avg_loss = total_loss / len(data_loader)\n","    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n","    report = classification_report(actual_labels, predictions, output_dict=True)\n","    accuracy = report['accuracy']\n","\n","\n","    return avg_loss, perplexity, accuracy"],"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, data_loader, device):\n","    model.eval()\n","    predictions = []\n","    actual_labels = []\n","\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            predictions.extend(preds.cpu().tolist())\n","            actual_labels.extend(labels.cpu().tolist())\n","\n","    avg_loss = total_loss / len(data_loader)\n","    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n","\n","    # Get classification report\n","    report = classification_report(actual_labels, predictions, output_dict=True)\n","    accuracy = report['accuracy']\n","\n","    return report, accuracy, avg_loss, perplexity"],"metadata":{"id":"pU_-Qi1Z3PmD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MdH78OOJxiU-"},"source":["# Main Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lK53I_iyxiU_"},"source":["def train_bert(model, tokenizer, text_dict, hyperparam_dict):\n","\n","    # Load in hyperparams\n","    num_epochs = hyperparam_dict['num_epochs']\n","    batch_size = hyperparam_dict['batch_size']\n","    learning_rate = hyperparam_dict['learning_rate']\n","    warmup_steps = hyperparam_dict['warmup_steps']\n","    max_length = hyperparam_dict['max_length']\n","\n","    # Load in text\n","    train_texts = text_dict['train_texts']\n","    val_texts = text_dict['val_texts']\n","    train_labels = text_dict['train_labels']\n","    val_labels = text_dict['val_labels']\n","\n","    # Tokenize the text from Training and Validation sets\n","    train_set = TokenizeDataset(train_texts, train_labels, tokenizer, max_length)\n","    val_set = TokenizeDataset(val_texts, val_labels, tokenizer, max_length)\n","\n","    # Utilize PyTorch's DataLoader to pass minibatches and reshuffle the data\n","    # at every epoch to reduce model overfitting; this approach uses\n","    # Pythonâ€™s multiprocessing to speed up data retrieval.\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_set, batch_size=batch_size)\n","\n","    # Initialize optimizer and scheduler\n","    optimizer = AdamW(model.parameters(), lr=learning_rate)\n","    total_steps = len(train_loader) * num_epochs\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n","                                                num_training_steps=total_steps)\n","\n","    train_losses = []\n","    train_perplexities = []\n","    train_accuracies = []\n","    val_losses = []\n","    val_perplexities = []\n","    val_accuracies = []\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch + 1}/{num_epochs}')\n","        train_loss, train_perplexity, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n","\n","        train_losses.append(train_loss)\n","        train_perplexities.append(train_perplexity)\n","        train_accuracies.append(train_acc)\n","\n","        print(f'Average train loss: {train_loss:.4f}')\n","        print(f'Train perplexity: {train_perplexity:.4f}')\n","\n","        # Evaluation\n","        print('\\nValidation Results:')\n","        val_report, val_acc, val_loss, val_perplexity = evaluate_model(model, val_set, device)\n","        print(val_report)\n","        print('-' * 60)\n","        print('Avg Acc:', val_acc)\n","\n","        val_losses.append(val_loss)\n","        val_perplexities.append(val_perplexity)\n","        val_accuracies.append(val_acc)\n","\n","    # Plot\n","    plot_training_metrics(train_losses, val_losses,\n","                          train_perplexities, val_perplexities,\n","                          train_accuracies, val_accuracies, num_epochs)\n","\n","    return model, tokenizer"],"outputs":[]},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"S_eMBCmKbS0Q"}},{"cell_type":"code","source":["# Load the dataset\n","ade_cl_df = pd.read_parquet(\"hf://datasets/ade-benchmark-corpus/ade_corpus_v2/Ade_corpus_v2_classification/train-00000-of-00001.parquet\")\n","\n","# Display basic information about the dataset\n","print(\"Dataset Shape:\", ade_cl_df.shape)\n","print(\"\\nSample of the data:\")\n","display(ade_cl_df.head())\n","print(\"\\nClass distribution:\")\n","display(ade_cl_df['label'].value_counts())"],"metadata":{"id":"cBbafbiabRUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIMVEMtWxiU_"},"source":["# Train the Model"]},{"cell_type":"code","source":["# Define what model we are fine-tuning.\n","model_path = \"medicalai/ClinicalBERT\"\n","tokenizer_path = \"medicalai/ClinicalBERT\"\n","\n","# Assumes we are using LLMs that have alredy been pretrained.\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForSequenceClassification.from_pretrained(tokenizer_path, num_labels=2).to(device)"],"metadata":{"id":"eO2Q5Fuza8mL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(ade_cl_df['text'].values,\n","                                                                    ade_cl_df['label'].values,\n","                                                                    test_size=0.2,\n","                                                                    random_state=42)\n","\n","text_dict = {'train_texts': train_texts, 'val_texts': val_texts, 'train_labels': train_labels, 'val_labels': val_labels}"],"metadata":{"id":"GsQ-C8OVdshD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Train"],"metadata":{"id":"4tWtMmufaoui"}},{"cell_type":"code","source":["hyperparam_dict = {'max_length': 128,        # Maximum sequence length\\n\",\n","                   'num_labels': 2,          # Binary classification (ADE vs non-ADE)\\n\",\n","                   'num_epochs': 10,          # Number of training epochs\\n\",\n","                   'batch_size': 64,         # Batch size for training\\n\",\n","                   'learning_rate': 5e-6,    # Learning rate for optimizer\\n\",\n","                   'warmup_steps': 100,\n","                   'weight_decay': 0.01 # Number of warmup steps for scheduler\\n\",\n","                   }"],"metadata":{"id":"yrl3taU3nn5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xrCwWYrxiVA"},"source":["# Train the model\n","trained_model, trained_tokenizer = train_bert(model, tokenizer, text_dict, hyperparam_dict)\n","\n","# Save the model and tokenizer\n","trained_model.save_pretrained(\"tiny_bert_ade_classifier\")\n","trained_tokenizer.save_pretrained(\"tiny_bert_ade_classifier\")\n","print(\"Model and tokenizer saved successfully!\")"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"svaBG4asxiVB"},"source":["# Trained Model Demonstration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQHE4IWsxiVB"},"source":["def predict_ade(text, model, tokenizer):\n","    # Prepare the text\n","    encoding = tokenizer(\n","        text,\n","        add_special_tokens=True,\n","        max_length=128,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='pt'\n","    )\n","\n","    # Move to device and get prediction\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        _, preds = torch.max(outputs.logits, dim=1)\n","\n","    return \"ADE\" if preds.item() == 1 else \"Not ADE\""],"outputs":[]},{"cell_type":"code","source":["# Example usage\n","example_text = \"The patient experienced severe headache after taking aspirin.\"\n","prediction = predict_ade(example_text, trained_model, trained_tokenizer)\n","print(f\"Text: {example_text}\")\n","print(f\"Prediction: {prediction}\")"],"metadata":{"id":"VGVnIQGy3Vx2"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}